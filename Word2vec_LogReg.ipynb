{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import csv\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import string\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet    label\n",
      "0  RT @Papapishu: Man it would fucking rule if we...  hateful\n",
      "1  It is time to draw close to Him &#128591;&#127...   normal\n",
      "2  if you notice me start to act different or dis...   normal\n",
      "3  Forget unfollowers, I believe in growing. 7 ne...   normal\n",
      "4  RT @Vitiligoprince: Hate Being sexually Frustr...  hateful\n",
      "5  Topped the group in TGP Disc Jam Season 2! Ont...   normal\n",
      "6  That daily baby aspirin for your #heart just m...   normal\n",
      "7  I liked a @YouTube video from @mattshea https:...   normal\n",
      "8  RT @LestuhGang_: If your fucking up &amp; your...  hateful\n",
      "9  @Move_Fwd give up. You've lost. You will not c...   normal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85966"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = 'C:\\\\Users\\\\NUREFSHINE\\\\Desktop\\\\2019Spring-3rdYear-2ndSemester\\\\CS464-2-Introduction to Machine Learning\\\\ML-proje\\\\'\n",
    "csv_path = os.path.join(root, 'hatespeech_text_abusive.xlsx')\n",
    "df = pd.read_excel(csv_path, header = None)\n",
    "df.rename(columns={0:'tweet', 1:'label'}, inplace=True)\n",
    "print(df.head(10))\n",
    "\n",
    "my_labels = ['hateful', 'normal']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"bitch\" in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man would fucking rule party perpetual warfare\n",
      "Label: hateful\n",
      "time draw close father draw near always\n",
      "Label: normal\n",
      "notice start act different distant something notice difference act\n",
      "Label: normal\n",
      "forget believe growing new last day via\n",
      "Label: normal\n",
      "hate sexually like fuck ion fuck anybody\n",
      "Label: hateful\n",
      "topped group disc jam season onto\n",
      "Label: normal\n",
      "daily baby aspirin might colon\n",
      "Label: normal\n",
      "video blue army coming ancient warfare\n",
      "Label: normal\n",
      "fucking dont tell fucking\n",
      "Label: hateful\n",
      "give lost convince one iota read conspiracy\n",
      "Label: normal\n",
      "one way fucking threw candy ever\n",
      "Label: hateful\n",
      "fucking\n",
      "Label: hateful\n",
      "struggle remember song days remember nearly\n",
      "Label: normal\n",
      "nasty ass freak like\n",
      "Label: hateful\n",
      "tried new cure common cold\n",
      "Label: normal\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print_plot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['tweet', 'label']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Label:', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet    label\n",
      "0     man would fucking rule party perpetual warfare  hateful\n",
      "1            time draw close father draw near always   normal\n",
      "2  notice start act different distant something n...   normal\n",
      "3            forget believe growing new last day via   normal\n",
      "4           hate sexually like fuck ion fuck anybody  hateful\n",
      "5                  topped group disc jam season onto   normal\n",
      "6                     daily baby aspirin might colon   normal\n",
      "7             video blue army coming ancient warfare   normal\n",
      "8                          fucking dont tell fucking  hateful\n",
      "9        give lost convince one iota read conspiracy   normal\n",
      "forget believe growing new last day via\n",
      "Label: normal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84865"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWORDS.append(\"rt\")\n",
    "\n",
    "WORDS = set(nltk.corpus.words.words())\n",
    "WORDS.update({\"fuck\", \"fucking\"})\n",
    "\n",
    "def clean_text():\n",
    "    df[\"tweet\"] = df[\"tweet\"].apply(lambda x: x.lower())\n",
    "    df[\"tweet\"] = [re.sub('(@[^\\s]+)|(#[^\\s]+)', '', tweet) for tweet in df[\"tweet\"]]\n",
    "    df[\"tweet\"] = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) for tweet in df[\"tweet\"]]\n",
    "    df[\"tweet\"] = [re.sub('(\\'[^\\s]+)|(&[^\\s]+)','',tweet) for tweet in df[\"tweet\"]]\n",
    "    df[\"tweet\"] = [re.sub('[^\\w\\s/:%.,_-]','',tweet) for tweet in df[\"tweet\"]]\n",
    "    df[\"tweet\"] = df[\"tweet\"].apply(lambda tweet: tweet.translate(str.maketrans('', '', string.punctuation)))\n",
    "    df[\"tweet\"] = df[\"tweet\"].apply(lambda tweet: tweet.translate(str.maketrans('', '', \"0123456789❤♀️♥⚽️《\")) )\n",
    "    df[\"tweet\"] = df[\"tweet\"].str.split(' ').apply(lambda tweet: ' '.join(k for k in tweet if k not in STOPWORDS))\n",
    "    df[\"tweet\"] = df[\"tweet\"].str.split(' ').apply(lambda tweet: ' '.join(k for k in tweet if k in WORDS))\n",
    "    df[\"tweet\"] = df[\"tweet\"].str.replace(' +', ' ', case=False)\n",
    "    df[\"tweet\"] = df[\"tweet\"].str.strip()\n",
    "    df[\"tweet\"].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=[\"tweet\"], inplace=True)\n",
    "      \n",
    "clean_text()\n",
    "    \n",
    "print(df.head(10))\n",
    "print_plot(3)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_path = os.path.join(root, 'GoogleNews-vectors-negative300.bin.gz')\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nurefshine\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['nake']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['beira']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['romero']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['serin']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['learnt']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['mashallah']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['arse']\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['tweet']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['tweet']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nurefshine\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal' 'hateful' 'hateful' ... 'normal' 'hateful' 'hateful']\n",
      "accuracy 0.8858208955223881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     hateful       0.89      0.79      0.84      9403\n",
      "      normal       0.88      0.94      0.91     16057\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     25460\n",
      "   macro avg       0.89      0.87      0.87     25460\n",
      "weighted avg       0.89      0.89      0.88     25460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "\n",
    "print (y_pred)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.label))\n",
    "print(classification_report(test.label, y_pred, target_names = my_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
